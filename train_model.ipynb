{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2fUwme2-yoI"
   },
   "source": [
    "This script is aiming to find the proper script to make a classicification of the images.\n",
    "\n",
    "# Import pacakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MaQ3V7q_JH6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import PIL\n",
    "#import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "# Use in the model\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2hNScPB_hd3"
   },
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PiEUOZ-0iGJw"
   },
   "outputs": [],
   "source": [
    "# Get the name of all the labels\n",
    "home_path = '/home/bowen/Documents/Ecomed/'\n",
    "data_path = '/home/bowen/Documents/Ecomed/dataset'\n",
    "# class_names = os.listdir(data_path)\n",
    "class_names = [\"background\", \"pharmaceutical\", \"sharps\", \"trace_chemo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4502 images belonging to 4 classes.\n",
      "Found 4501 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set up data generators that can read images from our dataset into Keras.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.5)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        data_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=24,\n",
    "        class_mode='binary',\n",
    "        subset='training')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        data_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=24,\n",
    "        class_mode='binary',\n",
    "        subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Show several images\n",
    "# data, labels = validation_generator.next()\n",
    "\n",
    "# for i, sample in enumerate(data):\n",
    "#     plt.imshow(sample)\n",
    "#     print(class_names[int(labels[i])])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gv-JUmSdAGHp"
   },
   "source": [
    "# MobileNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_index = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2410
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2084,
     "status": "ok",
     "timestamp": 1550511319795,
     "user": {
      "displayName": "Bowen Zhang",
      "photoUrl": "https://lh5.googleusercontent.com/-l5MP4uGzaQc/AAAAAAAAAAI/AAAAAAAAAAc/sFxE7kUGmvM/s64/photo.jpg",
      "userId": "01168430711303576671"
     },
     "user_tz": 300
    },
    "id": "DY2DaeG6_xIR",
    "outputId": "0729fd02-0ae4-4b8a-d1c9-c3da8d47fcde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import mobilenet\n",
    "# with tf.device('/gpu:' + str(gpu_index)):\n",
    "mobilenet = tf.keras.applications.MobileNet(include_top=False, input_shape=[224, 224, 3], weights='imagenet')\n",
    "mobilenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 5,854,916\n",
      "Trainable params: 5,833,028\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add new output layers.\n",
    "# with tf.device('/gpu:' + str(gpu_index)):\n",
    "x = mobilenet.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "\n",
    "# Combine feature layers with our new output layers.\n",
    "model = tf.keras.Model(inputs=mobilenet.input, outputs=preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2lwdStaxAO1z"
   },
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1694926,
     "status": "ok",
     "timestamp": 1550513025124,
     "user": {
      "displayName": "Bowen Zhang",
      "photoUrl": "https://lh5.googleusercontent.com/-l5MP4uGzaQc/AAAAAAAAAAI/AAAAAAAAAAc/sFxE7kUGmvM/s64/photo.jpg",
      "userId": "01168430711303576671"
     },
     "user_tz": 300
    },
    "id": "XgZH4ZHNAIQ0",
    "outputId": "18bc6d7f-39be-4671-acd3-4cd939b5227f"
   },
   "outputs": [],
   "source": [
    "# defintion of optimizer\n",
    "# with tf.device('/gpu:' + str(gpu_index)):\n",
    "# opt = tf.keras.optimizers.SGD(0.001)\n",
    "opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1694926,
     "status": "ok",
     "timestamp": 1550513025124,
     "user": {
      "displayName": "Bowen Zhang",
      "photoUrl": "https://lh5.googleusercontent.com/-l5MP4uGzaQc/AAAAAAAAAAI/AAAAAAAAAAc/sFxE7kUGmvM/s64/photo.jpg",
      "userId": "01168430711303576671"
     },
     "user_tz": 300
    },
    "id": "XgZH4ZHNAIQ0",
    "outputId": "18bc6d7f-39be-4671-acd3-4cd939b5227f"
   },
   "outputs": [],
   "source": [
    "# definition of checkpoint\n",
    "# with tf.device('/gpu:' + str(gpu_index)):\n",
    "checkpoint_path = os.path.join(home_path, 'models', 'model_4classes', \"model_4_classes.h5\")\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model(os.path.join(home_path + 'models', \"model_new_layer.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaRCvK_7ARDU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "188/188 [==============================] - 143s 760ms/step - loss: 1.4897 - acc: 0.3659 - val_loss: 1.1517 - val_acc: 0.4772\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 135s 716ms/step - loss: 0.9886 - acc: 0.5825 - val_loss: 0.7101 - val_acc: 0.7472\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 135s 717ms/step - loss: 0.6252 - acc: 0.7566 - val_loss: 0.3837 - val_acc: 0.8518\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 135s 718ms/step - loss: 0.4187 - acc: 0.8462 - val_loss: 0.3250 - val_acc: 0.8616\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 136s 721ms/step - loss: 0.2932 - acc: 0.8977 - val_loss: 0.2883 - val_acc: 0.8796\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 136s 724ms/step - loss: 0.1995 - acc: 0.9371 - val_loss: 0.2829 - val_acc: 0.8825\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 136s 721ms/step - loss: 0.1683 - acc: 0.9438 - val_loss: 0.2541 - val_acc: 0.8938\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 136s 722ms/step - loss: 0.1396 - acc: 0.9551 - val_loss: 0.2490 - val_acc: 0.9014\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 136s 721ms/step - loss: 0.1333 - acc: 0.9543 - val_loss: 0.2143 - val_acc: 0.9147\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 136s 721ms/step - loss: 0.1038 - acc: 0.9656 - val_loss: 0.2245 - val_acc: 0.9134\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 135s 717ms/step - loss: 0.0836 - acc: 0.9749 - val_loss: 0.2058 - val_acc: 0.9273\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 135s 718ms/step - loss: 0.0764 - acc: 0.9770 - val_loss: 0.2003 - val_acc: 0.9318\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 135s 716ms/step - loss: 0.0733 - acc: 0.9765 - val_loss: 0.1884 - val_acc: 0.9338\n",
      "Epoch 14/200\n",
      "188/188 [==============================] - 136s 722ms/step - loss: 0.0651 - acc: 0.9818 - val_loss: 0.1830 - val_acc: 0.9402\n",
      "Epoch 15/200\n",
      "188/188 [==============================] - 135s 719ms/step - loss: 0.0590 - acc: 0.9834 - val_loss: 0.1868 - val_acc: 0.9405\n",
      "Epoch 16/200\n",
      "188/188 [==============================] - 135s 716ms/step - loss: 0.0576 - acc: 0.9849 - val_loss: 0.1802 - val_acc: 0.9413\n",
      "Epoch 17/200\n",
      "188/188 [==============================] - 135s 716ms/step - loss: 0.0530 - acc: 0.9836 - val_loss: 0.1713 - val_acc: 0.9445\n",
      "Epoch 18/200\n",
      "188/188 [==============================] - 135s 720ms/step - loss: 0.0457 - acc: 0.9854 - val_loss: 0.1652 - val_acc: 0.9471\n",
      "Epoch 19/200\n",
      "188/188 [==============================] - 134s 711ms/step - loss: 0.0383 - acc: 0.9887 - val_loss: 0.1671 - val_acc: 0.9467\n",
      "Epoch 20/200\n",
      "188/188 [==============================] - 136s 722ms/step - loss: 0.0454 - acc: 0.9852 - val_loss: 0.1547 - val_acc: 0.9527\n",
      "Epoch 21/200\n",
      "188/188 [==============================] - 137s 728ms/step - loss: 0.0403 - acc: 0.9870 - val_loss: 0.1454 - val_acc: 0.9587\n",
      "Epoch 22/200\n",
      "188/188 [==============================] - 136s 722ms/step - loss: 0.0335 - acc: 0.9902 - val_loss: 0.1452 - val_acc: 0.9591\n",
      "Epoch 23/200\n",
      "188/188 [==============================] - 135s 716ms/step - loss: 0.0347 - acc: 0.9907 - val_loss: 0.1476 - val_acc: 0.9553\n",
      "Epoch 24/200\n",
      "188/188 [==============================] - 135s 720ms/step - loss: 0.0281 - acc: 0.9918 - val_loss: 0.1537 - val_acc: 0.9522\n",
      "Epoch 25/200\n",
      "188/188 [==============================] - 115s 611ms/step - loss: 0.0319 - acc: 0.9911 - val_loss: 0.1503 - val_acc: 0.9569\n",
      "Epoch 26/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0329 - acc: 0.9894 - val_loss: 0.1449 - val_acc: 0.9578\n",
      "Epoch 27/200\n",
      "188/188 [==============================] - 97s 515ms/step - loss: 0.0302 - acc: 0.9914 - val_loss: 0.1556 - val_acc: 0.9567\n",
      "Epoch 28/200\n",
      "188/188 [==============================] - 97s 517ms/step - loss: 0.0242 - acc: 0.9929 - val_loss: 0.1423 - val_acc: 0.9616\n",
      "Epoch 29/200\n",
      "188/188 [==============================] - 97s 515ms/step - loss: 0.0230 - acc: 0.9931 - val_loss: 0.1479 - val_acc: 0.9571\n",
      "Epoch 30/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0281 - acc: 0.9920 - val_loss: 0.1416 - val_acc: 0.9609\n",
      "Epoch 31/200\n",
      "188/188 [==============================] - 96s 512ms/step - loss: 0.0239 - acc: 0.9934 - val_loss: 0.1482 - val_acc: 0.9605\n",
      "Epoch 32/200\n",
      "188/188 [==============================] - 97s 517ms/step - loss: 0.0191 - acc: 0.9938 - val_loss: 0.1439 - val_acc: 0.9596\n",
      "Epoch 33/200\n",
      "188/188 [==============================] - 98s 521ms/step - loss: 0.0262 - acc: 0.9931 - val_loss: 0.1333 - val_acc: 0.9660\n",
      "Epoch 34/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0150 - acc: 0.9965 - val_loss: 0.1263 - val_acc: 0.9673\n",
      "Epoch 35/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0147 - acc: 0.9960 - val_loss: 0.1329 - val_acc: 0.9631\n",
      "Epoch 36/200\n",
      "188/188 [==============================] - 96s 511ms/step - loss: 0.0165 - acc: 0.9951 - val_loss: 0.1312 - val_acc: 0.9636\n",
      "Epoch 37/200\n",
      "188/188 [==============================] - 98s 519ms/step - loss: 0.0186 - acc: 0.9945 - val_loss: 0.1290 - val_acc: 0.9667\n",
      "Epoch 38/200\n",
      "188/188 [==============================] - 97s 514ms/step - loss: 0.0210 - acc: 0.9934 - val_loss: 0.1232 - val_acc: 0.9669\n",
      "Epoch 39/200\n",
      "188/188 [==============================] - 96s 511ms/step - loss: 0.0214 - acc: 0.9945 - val_loss: 0.1328 - val_acc: 0.9658\n",
      "Epoch 40/200\n",
      "188/188 [==============================] - 97s 519ms/step - loss: 0.0175 - acc: 0.9953 - val_loss: 0.1205 - val_acc: 0.9660\n",
      "Epoch 41/200\n",
      "188/188 [==============================] - 99s 524ms/step - loss: 0.0152 - acc: 0.9954 - val_loss: 0.1242 - val_acc: 0.9678\n",
      "Epoch 42/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0156 - acc: 0.9960 - val_loss: 0.1317 - val_acc: 0.9658\n",
      "Epoch 43/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0175 - acc: 0.9960 - val_loss: 0.1248 - val_acc: 0.9680\n",
      "Epoch 44/200\n",
      "188/188 [==============================] - 98s 522ms/step - loss: 0.0173 - acc: 0.9953 - val_loss: 0.1135 - val_acc: 0.9705\n",
      "Epoch 45/200\n",
      "188/188 [==============================] - 97s 514ms/step - loss: 0.0121 - acc: 0.9973 - val_loss: 0.1201 - val_acc: 0.9667\n",
      "Epoch 46/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0136 - acc: 0.9965 - val_loss: 0.1204 - val_acc: 0.9705\n",
      "Epoch 47/200\n",
      "188/188 [==============================] - 96s 513ms/step - loss: 0.0172 - acc: 0.9958 - val_loss: 0.1201 - val_acc: 0.9705\n",
      "Epoch 48/200\n",
      "188/188 [==============================] - 96s 511ms/step - loss: 0.0186 - acc: 0.9962 - val_loss: 0.1134 - val_acc: 0.9702\n",
      "Epoch 49/200\n",
      "188/188 [==============================] - 96s 511ms/step - loss: 0.0127 - acc: 0.9971 - val_loss: 0.1199 - val_acc: 0.9713\n",
      "Epoch 50/200\n",
      "188/188 [==============================] - 98s 522ms/step - loss: 0.0154 - acc: 0.9953 - val_loss: 0.1132 - val_acc: 0.9731\n",
      "Epoch 51/200\n",
      "188/188 [==============================] - 98s 519ms/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.1123 - val_acc: 0.9698\n",
      "Epoch 52/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0125 - acc: 0.9958 - val_loss: 0.1169 - val_acc: 0.9691\n",
      "Epoch 53/200\n",
      "188/188 [==============================] - 96s 509ms/step - loss: 0.0135 - acc: 0.9953 - val_loss: 0.1185 - val_acc: 0.9702\n",
      "Epoch 54/200\n",
      "188/188 [==============================] - 97s 519ms/step - loss: 0.0142 - acc: 0.9969 - val_loss: 0.1156 - val_acc: 0.9713\n",
      "Epoch 55/200\n",
      "188/188 [==============================] - 98s 522ms/step - loss: 0.0097 - acc: 0.9978 - val_loss: 0.1082 - val_acc: 0.9731\n",
      "Epoch 56/200\n",
      "188/188 [==============================] - 98s 523ms/step - loss: 0.0139 - acc: 0.9962 - val_loss: 0.1220 - val_acc: 0.9682\n",
      "Epoch 57/200\n",
      "188/188 [==============================] - 97s 517ms/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.1004 - val_acc: 0.9725\n",
      "Epoch 58/200\n",
      "188/188 [==============================] - 98s 519ms/step - loss: 0.0107 - acc: 0.9973 - val_loss: 0.1147 - val_acc: 0.9722\n",
      "Epoch 59/200\n",
      "188/188 [==============================] - 97s 515ms/step - loss: 0.0125 - acc: 0.9971 - val_loss: 0.1131 - val_acc: 0.9718\n",
      "Epoch 60/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0077 - acc: 0.9982 - val_loss: 0.1121 - val_acc: 0.9729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0073 - acc: 0.9980 - val_loss: 0.1091 - val_acc: 0.9753\n",
      "Epoch 62/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0100 - acc: 0.9967 - val_loss: 0.1037 - val_acc: 0.9760\n",
      "Epoch 63/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0106 - acc: 0.9971 - val_loss: 0.1128 - val_acc: 0.9753\n",
      "Epoch 64/200\n",
      "188/188 [==============================] - 97s 517ms/step - loss: 0.0112 - acc: 0.9967 - val_loss: 0.1077 - val_acc: 0.9756\n",
      "Epoch 65/200\n",
      "188/188 [==============================] - 98s 519ms/step - loss: 0.0088 - acc: 0.9965 - val_loss: 0.1056 - val_acc: 0.9756\n",
      "Epoch 66/200\n",
      "188/188 [==============================] - 96s 513ms/step - loss: 0.0101 - acc: 0.9980 - val_loss: 0.0997 - val_acc: 0.9756\n",
      "Epoch 67/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0082 - acc: 0.9978 - val_loss: 0.0993 - val_acc: 0.9760\n",
      "Epoch 68/200\n",
      "188/188 [==============================] - 97s 515ms/step - loss: 0.0090 - acc: 0.9971 - val_loss: 0.0999 - val_acc: 0.9756\n",
      "Epoch 69/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0967 - val_acc: 0.9769\n",
      "Epoch 70/200\n",
      "188/188 [==============================] - 97s 515ms/step - loss: 0.0079 - acc: 0.9976 - val_loss: 0.1025 - val_acc: 0.9742\n",
      "Epoch 71/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0095 - acc: 0.9973 - val_loss: 0.1096 - val_acc: 0.9738\n",
      "Epoch 72/200\n",
      "188/188 [==============================] - 97s 517ms/step - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0989 - val_acc: 0.9758\n",
      "Epoch 73/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0076 - acc: 0.9971 - val_loss: 0.1029 - val_acc: 0.9769\n",
      "Epoch 74/200\n",
      "188/188 [==============================] - 97s 519ms/step - loss: 0.0132 - acc: 0.9960 - val_loss: 0.0948 - val_acc: 0.9780\n",
      "Epoch 75/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.0939 - val_acc: 0.9762\n",
      "Epoch 76/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0083 - acc: 0.9980 - val_loss: 0.0976 - val_acc: 0.9789\n",
      "Epoch 77/200\n",
      "188/188 [==============================] - 96s 512ms/step - loss: 0.0086 - acc: 0.9976 - val_loss: 0.0939 - val_acc: 0.9798\n",
      "Epoch 78/200\n",
      "188/188 [==============================] - 97s 514ms/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.0916 - val_acc: 0.9769\n",
      "Epoch 79/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0071 - acc: 0.9980 - val_loss: 0.0866 - val_acc: 0.9796\n",
      "Epoch 80/200\n",
      "188/188 [==============================] - 98s 521ms/step - loss: 0.0093 - acc: 0.9961 - val_loss: 0.1009 - val_acc: 0.9771\n",
      "Epoch 81/200\n",
      "188/188 [==============================] - 98s 521ms/step - loss: 0.0075 - acc: 0.9976 - val_loss: 0.0823 - val_acc: 0.9809\n",
      "Epoch 82/200\n",
      "188/188 [==============================] - 97s 515ms/step - loss: 0.0052 - acc: 0.9989 - val_loss: 0.0978 - val_acc: 0.9791\n",
      "Epoch 83/200\n",
      "188/188 [==============================] - 98s 519ms/step - loss: 0.0118 - acc: 0.9958 - val_loss: 0.0867 - val_acc: 0.9818\n",
      "Epoch 84/200\n",
      "188/188 [==============================] - 98s 519ms/step - loss: 0.0093 - acc: 0.9973 - val_loss: 0.0885 - val_acc: 0.9791\n",
      "Epoch 85/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0064 - acc: 0.9980 - val_loss: 0.0998 - val_acc: 0.9776\n",
      "Epoch 86/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0079 - acc: 0.9982 - val_loss: 0.0998 - val_acc: 0.9787\n",
      "Epoch 87/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0071 - acc: 0.9980 - val_loss: 0.0980 - val_acc: 0.9787\n",
      "Epoch 88/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0061 - acc: 0.9989 - val_loss: 0.0909 - val_acc: 0.9796\n",
      "Epoch 89/200\n",
      "188/188 [==============================] - 98s 521ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.0967 - val_acc: 0.9769\n",
      "Epoch 90/200\n",
      "188/188 [==============================] - 97s 514ms/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.0923 - val_acc: 0.9791\n",
      "Epoch 91/200\n",
      "188/188 [==============================] - 96s 511ms/step - loss: 0.0068 - acc: 0.9984 - val_loss: 0.0898 - val_acc: 0.9787\n",
      "Epoch 92/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0063 - acc: 0.9984 - val_loss: 0.0980 - val_acc: 0.9769\n",
      "Epoch 93/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0043 - acc: 0.9987 - val_loss: 0.0974 - val_acc: 0.9787\n",
      "Epoch 94/200\n",
      "188/188 [==============================] - 96s 510ms/step - loss: 0.0059 - acc: 0.9976 - val_loss: 0.1038 - val_acc: 0.9767\n",
      "Epoch 95/200\n",
      "188/188 [==============================] - 98s 519ms/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.0880 - val_acc: 0.9802\n",
      "Epoch 96/200\n",
      "188/188 [==============================] - 97s 515ms/step - loss: 0.0053 - acc: 0.9984 - val_loss: 0.0931 - val_acc: 0.9800\n",
      "Epoch 97/200\n",
      "188/188 [==============================] - 97s 514ms/step - loss: 0.0064 - acc: 0.9978 - val_loss: 0.0914 - val_acc: 0.9784\n",
      "Epoch 98/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0045 - acc: 0.9984 - val_loss: 0.0974 - val_acc: 0.9789\n",
      "Epoch 99/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0902 - val_acc: 0.9813\n",
      "Epoch 100/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0083 - acc: 0.9980 - val_loss: 0.0882 - val_acc: 0.9800\n",
      "Epoch 101/200\n",
      "188/188 [==============================] - 97s 516ms/step - loss: 0.0050 - acc: 0.9982 - val_loss: 0.0887 - val_acc: 0.9804\n",
      "Epoch 102/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0063 - acc: 0.9982 - val_loss: 0.1027 - val_acc: 0.9776\n",
      "Epoch 103/200\n",
      "188/188 [==============================] - 97s 517ms/step - loss: 0.0043 - acc: 0.9985 - val_loss: 0.0936 - val_acc: 0.9796\n",
      "Epoch 104/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0065 - acc: 0.9978 - val_loss: 0.0852 - val_acc: 0.9800\n",
      "Epoch 105/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0073 - acc: 0.9980 - val_loss: 0.0996 - val_acc: 0.9784\n",
      "Epoch 106/200\n",
      "188/188 [==============================] - 97s 515ms/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0953 - val_acc: 0.9804\n",
      "Epoch 107/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.0950 - val_acc: 0.9793\n",
      "Epoch 108/200\n",
      "188/188 [==============================] - 96s 511ms/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.0964 - val_acc: 0.9773\n",
      "Epoch 109/200\n",
      "188/188 [==============================] - 97s 515ms/step - loss: 0.0032 - acc: 0.9991 - val_loss: 0.1010 - val_acc: 0.9769\n",
      "Epoch 110/200\n",
      "188/188 [==============================] - 97s 514ms/step - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0825 - val_acc: 0.9811\n",
      "Epoch 111/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0039 - acc: 0.9984 - val_loss: 0.0923 - val_acc: 0.9804\n",
      "Epoch 112/200\n",
      "188/188 [==============================] - 97s 514ms/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.0864 - val_acc: 0.9824\n",
      "Epoch 113/200\n",
      "188/188 [==============================] - 97s 517ms/step - loss: 0.0039 - acc: 0.9991 - val_loss: 0.0880 - val_acc: 0.9800\n",
      "Epoch 114/200\n",
      "188/188 [==============================] - 97s 514ms/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.0864 - val_acc: 0.9796\n",
      "Epoch 115/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0034 - acc: 0.9996 - val_loss: 0.0963 - val_acc: 0.9784\n",
      "Epoch 116/200\n",
      "188/188 [==============================] - 97s 517ms/step - loss: 0.0042 - acc: 0.9984 - val_loss: 0.0879 - val_acc: 0.9798\n",
      "Epoch 117/200\n",
      "188/188 [==============================] - 98s 519ms/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0935 - val_acc: 0.9798\n",
      "Epoch 118/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0035 - acc: 0.9989 - val_loss: 0.0834 - val_acc: 0.9822\n",
      "Epoch 119/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0033 - acc: 0.9993 - val_loss: 0.0904 - val_acc: 0.9791\n",
      "Epoch 120/200\n",
      "188/188 [==============================] - 96s 512ms/step - loss: 0.0029 - acc: 0.9990 - val_loss: 0.0873 - val_acc: 0.9798\n",
      "Epoch 121/200\n",
      "188/188 [==============================] - 98s 520ms/step - loss: 0.0038 - acc: 0.9987 - val_loss: 0.0872 - val_acc: 0.9813\n",
      "Epoch 122/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0048 - acc: 0.9984 - val_loss: 0.0942 - val_acc: 0.9804\n",
      "Epoch 123/200\n",
      "188/188 [==============================] - 96s 511ms/step - loss: 0.0057 - acc: 0.9981 - val_loss: 0.0885 - val_acc: 0.9818\n",
      "Epoch 124/200\n",
      "188/188 [==============================] - 97s 518ms/step - loss: 0.0060 - acc: 0.9976 - val_loss: 0.0859 - val_acc: 0.9813\n",
      "Epoch 125/200\n",
      "188/188 [==============================] - 98s 521ms/step - loss: 0.0047 - acc: 0.9987 - val_loss: 0.0871 - val_acc: 0.9822\n",
      "Epoch 126/200\n",
      "188/188 [==============================] - 98s 522ms/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0933 - val_acc: 0.9802\n",
      "Epoch 127/200\n",
      "188/188 [==============================] - 143s 762ms/step - loss: 0.0045 - acc: 0.9982 - val_loss: 0.0952 - val_acc: 0.9818\n",
      "Epoch 128/200\n",
      "188/188 [==============================] - 153s 814ms/step - loss: 0.0056 - acc: 0.9984 - val_loss: 0.0944 - val_acc: 0.9813\n",
      "Epoch 129/200\n",
      "188/188 [==============================] - 146s 777ms/step - loss: 0.0038 - acc: 0.9990 - val_loss: 0.0844 - val_acc: 0.9818\n",
      "Epoch 130/200\n",
      "188/188 [==============================] - 157s 833ms/step - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0978 - val_acc: 0.9804\n",
      "Epoch 131/200\n",
      "188/188 [==============================] - 156s 830ms/step - loss: 0.0073 - acc: 0.9980 - val_loss: 0.0980 - val_acc: 0.9807\n",
      "Epoch 132/200\n",
      "188/188 [==============================] - 146s 777ms/step - loss: 0.0031 - acc: 0.9991 - val_loss: 0.0938 - val_acc: 0.9802\n",
      "Epoch 133/200\n",
      "188/188 [==============================] - 148s 786ms/step - loss: 0.0051 - acc: 0.9989 - val_loss: 0.0883 - val_acc: 0.9831\n",
      "Epoch 134/200\n",
      "188/188 [==============================] - 161s 858ms/step - loss: 0.0051 - acc: 0.9990 - val_loss: 0.0852 - val_acc: 0.9816\n",
      "Epoch 135/200\n",
      "188/188 [==============================] - 160s 854ms/step - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0960 - val_acc: 0.9809\n",
      "Epoch 136/200\n",
      "188/188 [==============================] - 151s 803ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.0981 - val_acc: 0.9813\n",
      "Epoch 137/200\n",
      "188/188 [==============================] - 154s 819ms/step - loss: 0.0034 - acc: 0.9991 - val_loss: 0.0812 - val_acc: 0.9820\n",
      "Epoch 138/200\n",
      "188/188 [==============================] - 163s 867ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.0934 - val_acc: 0.9804\n",
      "Epoch 139/200\n",
      "188/188 [==============================] - 162s 859ms/step - loss: 0.0040 - acc: 0.9984 - val_loss: 0.0851 - val_acc: 0.9824\n",
      "Epoch 140/200\n",
      "188/188 [==============================] - 164s 873ms/step - loss: 0.0051 - acc: 0.9984 - val_loss: 0.1012 - val_acc: 0.9791\n",
      "Epoch 141/200\n",
      "188/188 [==============================] - 146s 775ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.0963 - val_acc: 0.9789\n",
      "Epoch 142/200\n",
      "188/188 [==============================] - 155s 822ms/step - loss: 0.0025 - acc: 0.9993 - val_loss: 0.0784 - val_acc: 0.9820\n",
      "Epoch 143/200\n",
      "188/188 [==============================] - 208s 1s/step - loss: 0.0021 - acc: 0.9991 - val_loss: 0.0872 - val_acc: 0.9811\n",
      "Epoch 144/200\n",
      "188/188 [==============================] - 267s 1s/step - loss: 0.0020 - acc: 0.9993 - val_loss: 0.0948 - val_acc: 0.9802\n",
      "Epoch 145/200\n",
      "188/188 [==============================] - 270s 1s/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.0989 - val_acc: 0.9802\n",
      "Epoch 146/200\n",
      "188/188 [==============================] - 268s 1s/step - loss: 0.0042 - acc: 0.9989 - val_loss: 0.0825 - val_acc: 0.9824\n",
      "Epoch 147/200\n",
      "188/188 [==============================] - 271s 1s/step - loss: 0.0037 - acc: 0.9991 - val_loss: 0.0968 - val_acc: 0.9824\n",
      "Epoch 148/200\n",
      "188/188 [==============================] - 264s 1s/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.0923 - val_acc: 0.9820\n",
      "Epoch 149/200\n",
      "188/188 [==============================] - 265s 1s/step - loss: 0.0031 - acc: 0.9989 - val_loss: 0.0990 - val_acc: 0.9809\n",
      "Epoch 150/200\n",
      "188/188 [==============================] - 266s 1s/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.0846 - val_acc: 0.9816\n",
      "Epoch 151/200\n",
      "188/188 [==============================] - 265s 1s/step - loss: 0.0043 - acc: 0.9989 - val_loss: 0.0937 - val_acc: 0.9824\n",
      "Epoch 152/200\n",
      "188/188 [==============================] - 266s 1s/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0870 - val_acc: 0.9822\n",
      "Epoch 153/200\n",
      "188/188 [==============================] - 262s 1s/step - loss: 0.0023 - acc: 0.9996 - val_loss: 0.0955 - val_acc: 0.9798\n",
      "Epoch 154/200\n",
      "188/188 [==============================] - 261s 1s/step - loss: 0.0027 - acc: 0.9996 - val_loss: 0.0982 - val_acc: 0.9798\n",
      "Epoch 155/200\n",
      "188/188 [==============================] - 261s 1s/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0836 - val_acc: 0.9829\n",
      "Epoch 156/200\n",
      "188/188 [==============================] - 266s 1s/step - loss: 0.0029 - acc: 0.9987 - val_loss: 0.0950 - val_acc: 0.9836\n",
      "Epoch 157/200\n",
      "188/188 [==============================] - 261s 1s/step - loss: 0.0019 - acc: 0.9996 - val_loss: 0.0905 - val_acc: 0.9827\n",
      "Epoch 158/200\n",
      "188/188 [==============================] - 263s 1s/step - loss: 0.0024 - acc: 0.9993 - val_loss: 0.0961 - val_acc: 0.9818\n",
      "Epoch 159/200\n",
      "188/188 [==============================] - 263s 1s/step - loss: 0.0018 - acc: 0.9998 - val_loss: 0.0799 - val_acc: 0.9820\n",
      "Epoch 160/200\n",
      "188/188 [==============================] - 266s 1s/step - loss: 0.0025 - acc: 0.9989 - val_loss: 0.0975 - val_acc: 0.9811\n",
      "Epoch 161/200\n",
      "188/188 [==============================] - 265s 1s/step - loss: 0.0023 - acc: 0.9991 - val_loss: 0.1009 - val_acc: 0.9787\n",
      "Epoch 162/200\n",
      "188/188 [==============================] - 263s 1s/step - loss: 0.0040 - acc: 0.9993 - val_loss: 0.0855 - val_acc: 0.9831\n",
      "Epoch 163/200\n",
      "188/188 [==============================] - 268s 1s/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.0838 - val_acc: 0.9820\n",
      "Epoch 164/200\n",
      "188/188 [==============================] - 274s 1s/step - loss: 0.0025 - acc: 0.9991 - val_loss: 0.0949 - val_acc: 0.9836\n",
      "Epoch 165/200\n",
      "187/188 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9996"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-2a84b20ca88e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                         callbacks=[cp_callback])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 max_queue_size=max_queue_size)\n\u001b[0m\u001b[1;32m    198\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         raise ValueError('Output of generator should be a tuple '\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "with tf.device('/gpu:3'):\n",
    "    model.fit_generator(generator=train_generator,\n",
    "                        validation_data=validation_generator, \n",
    "                        epochs=200,\n",
    "                        callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: ['train']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/bowen/Documents/Ecomed/models/model_4classes/temp-b'1551934800'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'/home/bowen/Documents/Ecomed/models/model_4classes/1551934800'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.contrib.saved_model.save_keras_model(model, home_path + 'models/model_4classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_Model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
