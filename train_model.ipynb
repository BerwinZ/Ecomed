{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2fUwme2-yoI"
   },
   "source": [
    "This script is aiming to find the proper script to make a classicification of the images.\n",
    "\n",
    "# Import pacakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MaQ3V7q_JH6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "import PIL\n",
    "#import requests\n",
    "from io import BytesIO\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "# Use in the model\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2hNScPB_hd3"
   },
   "source": [
    "# Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PiEUOZ-0iGJw"
   },
   "outputs": [],
   "source": [
    "# Get the name of all the labels\n",
    "home_path = '/home/bowen/Documents/Ecomed/'\n",
    "data_path = '/home/bowen/Documents/Ecomed/dataset'\n",
    "class_names = os.listdir(data_path)\n",
    "class_names = [\"pharmaceutical\", \"sharps\", \"trace_chemo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3003 images belonging to 3 classes.\n",
      "Found 3001 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set up data generators that can read images from our dataset into Keras.\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.5)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        data_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=24,\n",
    "        class_mode='binary',\n",
    "        subset='training')\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        data_path,\n",
    "        target_size=(224, 224),\n",
    "        batch_size=24,\n",
    "        class_mode='binary',\n",
    "        subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Show several images\n",
    "# data, labels = validation_generator.next()\n",
    "\n",
    "# for i, sample in enumerate(data):\n",
    "#     plt.imshow(sample)\n",
    "#     print(class_names[int(labels[i])])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gv-JUmSdAGHp"
   },
   "source": [
    "# MobileNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2410
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2084,
     "status": "ok",
     "timestamp": 1550511319795,
     "user": {
      "displayName": "Bowen Zhang",
      "photoUrl": "https://lh5.googleusercontent.com/-l5MP4uGzaQc/AAAAAAAAAAI/AAAAAAAAAAc/sFxE7kUGmvM/s64/photo.jpg",
      "userId": "01168430711303576671"
     },
     "user_tz": 300
    },
    "id": "DY2DaeG6_xIR",
    "outputId": "0729fd02-0ae4-4b8a-d1c9-c3da8d47fcde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Import mobilenet\n",
    "mobilenet = tf.keras.applications.MobileNet(include_top=False, input_shape=[224, 224, 3], weights='imagenet')\n",
    "mobilenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 225, 225, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 112, 112, 32)      864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (ReLU)            (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 112, 112, 32)      288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (ReLU)        (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 112, 112, 64)      2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 112, 112, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (ReLU)        (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 113, 113, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 56, 56, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (ReLU)        (None, 56, 56, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 56, 56, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 56, 56, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 56, 56, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 56, 56, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (ReLU)        (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 57, 57, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 28, 28, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (ReLU)        (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 28, 28, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 28, 28, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 28, 28, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 28, 28, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (ReLU)        (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 29, 29, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 14, 14, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 14, 14, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (ReLU)        (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 14, 14, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (ReLU)        (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 14, 14, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 14, 14, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 14, 14, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (ReLU)       (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 7, 7, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 7, 7, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)       (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 7, 7, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 7, 7, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 7, 7, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 7, 7, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)       (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_5 ( (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 1539      \n",
      "=================================================================\n",
      "Total params: 5,854,403\n",
      "Trainable params: 5,832,515\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Add new output layers.\n",
    "x = mobilenet.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "preds = tf.keras.layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "# Combine feature layers with our new output layers.\n",
    "model = tf.keras.Model(inputs=mobilenet.input, outputs=preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2lwdStaxAO1z"
   },
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1694926,
     "status": "ok",
     "timestamp": 1550513025124,
     "user": {
      "displayName": "Bowen Zhang",
      "photoUrl": "https://lh5.googleusercontent.com/-l5MP4uGzaQc/AAAAAAAAAAI/AAAAAAAAAAc/sFxE7kUGmvM/s64/photo.jpg",
      "userId": "01168430711303576671"
     },
     "user_tz": 300
    },
    "id": "XgZH4ZHNAIQ0",
    "outputId": "18bc6d7f-39be-4671-acd3-4cd939b5227f"
   },
   "outputs": [],
   "source": [
    "# defintion of optimizer\n",
    "# opt = tf.keras.optimizers.SGD(0.001)\n",
    "opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 743
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1694926,
     "status": "ok",
     "timestamp": 1550513025124,
     "user": {
      "displayName": "Bowen Zhang",
      "photoUrl": "https://lh5.googleusercontent.com/-l5MP4uGzaQc/AAAAAAAAAAI/AAAAAAAAAAc/sFxE7kUGmvM/s64/photo.jpg",
      "userId": "01168430711303576671"
     },
     "user_tz": 300
    },
    "id": "XgZH4ZHNAIQ0",
    "outputId": "18bc6d7f-39be-4671-acd3-4cd939b5227f"
   },
   "outputs": [],
   "source": [
    "# definition of checkpoint\n",
    "checkpoint_path = os.path.join(home_path + 'models', \"model_success.h5\")\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model(os.path.join(home_path + 'models', \"model_new_layer.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaRCvK_7ARDU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "126/126 [==============================] - 93s 736ms/step - loss: 1.1731 - acc: 0.4693 - val_loss: 0.8963 - val_acc: 0.5721\n",
      "Epoch 2/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.8735 - acc: 0.6058 - val_loss: 0.6994 - val_acc: 0.7061\n",
      "Epoch 3/500\n",
      "126/126 [==============================] - 83s 661ms/step - loss: 0.6100 - acc: 0.7483 - val_loss: 0.5593 - val_acc: 0.7481\n",
      "Epoch 4/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.4484 - acc: 0.8281 - val_loss: 0.4698 - val_acc: 0.7871\n",
      "Epoch 5/500\n",
      "126/126 [==============================] - 82s 650ms/step - loss: 0.3098 - acc: 0.8806 - val_loss: 0.4651 - val_acc: 0.8027\n",
      "Epoch 6/500\n",
      "126/126 [==============================] - 84s 664ms/step - loss: 0.2370 - acc: 0.9183 - val_loss: 0.3915 - val_acc: 0.8354\n",
      "Epoch 7/500\n",
      "126/126 [==============================] - 83s 657ms/step - loss: 0.1829 - acc: 0.9431 - val_loss: 0.3640 - val_acc: 0.8437\n",
      "Epoch 8/500\n",
      "126/126 [==============================] - 82s 650ms/step - loss: 0.1581 - acc: 0.9454 - val_loss: 0.3372 - val_acc: 0.8717\n",
      "Epoch 9/500\n",
      "126/126 [==============================] - 83s 656ms/step - loss: 0.1347 - acc: 0.9527 - val_loss: 0.3072 - val_acc: 0.8814\n",
      "Epoch 10/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.1159 - acc: 0.9649 - val_loss: 0.3177 - val_acc: 0.8760\n",
      "Epoch 11/500\n",
      "126/126 [==============================] - 82s 653ms/step - loss: 0.1185 - acc: 0.9590 - val_loss: 0.3307 - val_acc: 0.8670\n",
      "Epoch 12/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0832 - acc: 0.9772 - val_loss: 0.3028 - val_acc: 0.8877\n",
      "Epoch 13/500\n",
      "126/126 [==============================] - 83s 656ms/step - loss: 0.0839 - acc: 0.9726 - val_loss: 0.2769 - val_acc: 0.9020\n",
      "Epoch 14/500\n",
      "126/126 [==============================] - 83s 657ms/step - loss: 0.0783 - acc: 0.9755 - val_loss: 0.2613 - val_acc: 0.9110\n",
      "Epoch 15/500\n",
      "126/126 [==============================] - 82s 653ms/step - loss: 0.0746 - acc: 0.9778 - val_loss: 0.2621 - val_acc: 0.9087\n",
      "Epoch 16/500\n",
      "126/126 [==============================] - 83s 656ms/step - loss: 0.0622 - acc: 0.9805 - val_loss: 0.3061 - val_acc: 0.8967\n",
      "Epoch 17/500\n",
      "126/126 [==============================] - 83s 655ms/step - loss: 0.0635 - acc: 0.9785 - val_loss: 0.2821 - val_acc: 0.8990\n",
      "Epoch 18/500\n",
      "126/126 [==============================] - 84s 665ms/step - loss: 0.0552 - acc: 0.9815 - val_loss: 0.3014 - val_acc: 0.9024\n",
      "Epoch 19/500\n",
      "126/126 [==============================] - 85s 676ms/step - loss: 0.0489 - acc: 0.9838 - val_loss: 0.2557 - val_acc: 0.9207\n",
      "Epoch 20/500\n",
      "126/126 [==============================] - 84s 667ms/step - loss: 0.0677 - acc: 0.9775 - val_loss: 0.2532 - val_acc: 0.9224\n",
      "Epoch 21/500\n",
      "126/126 [==============================] - 84s 666ms/step - loss: 0.0387 - acc: 0.9884 - val_loss: 0.2413 - val_acc: 0.9197\n",
      "Epoch 22/500\n",
      "126/126 [==============================] - 82s 650ms/step - loss: 0.0501 - acc: 0.9845 - val_loss: 0.2263 - val_acc: 0.9287\n",
      "Epoch 23/500\n",
      "126/126 [==============================] - 83s 660ms/step - loss: 0.0597 - acc: 0.9762 - val_loss: 0.2151 - val_acc: 0.9340\n",
      "Epoch 24/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.0439 - acc: 0.9845 - val_loss: 0.2234 - val_acc: 0.9280\n",
      "Epoch 25/500\n",
      "126/126 [==============================] - 83s 656ms/step - loss: 0.0506 - acc: 0.9845 - val_loss: 0.2444 - val_acc: 0.9247\n",
      "Epoch 26/500\n",
      "126/126 [==============================] - 83s 661ms/step - loss: 0.0406 - acc: 0.9861 - val_loss: 0.2002 - val_acc: 0.9427\n",
      "Epoch 27/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0422 - acc: 0.9878 - val_loss: 0.2083 - val_acc: 0.9407\n",
      "Epoch 28/500\n",
      "126/126 [==============================] - 83s 655ms/step - loss: 0.0314 - acc: 0.9911 - val_loss: 0.2456 - val_acc: 0.9207\n",
      "Epoch 29/500\n",
      "126/126 [==============================] - 82s 654ms/step - loss: 0.0375 - acc: 0.9881 - val_loss: 0.1892 - val_acc: 0.9450\n",
      "Epoch 30/500\n",
      "126/126 [==============================] - 83s 657ms/step - loss: 0.0290 - acc: 0.9934 - val_loss: 0.1799 - val_acc: 0.9490\n",
      "Epoch 31/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.0376 - acc: 0.9871 - val_loss: 0.2147 - val_acc: 0.9414\n",
      "Epoch 32/500\n",
      "126/126 [==============================] - 82s 653ms/step - loss: 0.0335 - acc: 0.9852 - val_loss: 0.2019 - val_acc: 0.9407\n",
      "Epoch 33/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0429 - acc: 0.9845 - val_loss: 0.2376 - val_acc: 0.9250\n",
      "Epoch 34/500\n",
      "126/126 [==============================] - 82s 651ms/step - loss: 0.0426 - acc: 0.9858 - val_loss: 0.2042 - val_acc: 0.9464\n",
      "Epoch 35/500\n",
      "126/126 [==============================] - 82s 651ms/step - loss: 0.0400 - acc: 0.9894 - val_loss: 0.1901 - val_acc: 0.9420\n",
      "Epoch 36/500\n",
      "126/126 [==============================] - 82s 651ms/step - loss: 0.0375 - acc: 0.9888 - val_loss: 0.1956 - val_acc: 0.9464\n",
      "Epoch 37/500\n",
      "126/126 [==============================] - 83s 655ms/step - loss: 0.0174 - acc: 0.9937 - val_loss: 0.1877 - val_acc: 0.9480\n",
      "Epoch 38/500\n",
      "126/126 [==============================] - 83s 655ms/step - loss: 0.0303 - acc: 0.9914 - val_loss: 0.1657 - val_acc: 0.9507\n",
      "Epoch 39/500\n",
      "126/126 [==============================] - 82s 653ms/step - loss: 0.0272 - acc: 0.9927 - val_loss: 0.2007 - val_acc: 0.9404\n",
      "Epoch 40/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.1729 - val_acc: 0.9497\n",
      "Epoch 41/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.0196 - acc: 0.9947 - val_loss: 0.1912 - val_acc: 0.9447\n",
      "Epoch 42/500\n",
      "126/126 [==============================] - 83s 657ms/step - loss: 0.0195 - acc: 0.9944 - val_loss: 0.2115 - val_acc: 0.9424\n",
      "Epoch 43/500\n",
      "126/126 [==============================] - 83s 657ms/step - loss: 0.0192 - acc: 0.9937 - val_loss: 0.1929 - val_acc: 0.9460\n",
      "Epoch 44/500\n",
      "126/126 [==============================] - 82s 654ms/step - loss: 0.0372 - acc: 0.9842 - val_loss: 0.1594 - val_acc: 0.9540\n",
      "Epoch 45/500\n",
      "126/126 [==============================] - 83s 656ms/step - loss: 0.0156 - acc: 0.9957 - val_loss: 0.1730 - val_acc: 0.9550\n",
      "Epoch 46/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0367 - acc: 0.9914 - val_loss: 0.1754 - val_acc: 0.9503\n",
      "Epoch 47/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0236 - acc: 0.9937 - val_loss: 0.1720 - val_acc: 0.9477\n",
      "Epoch 48/500\n",
      "126/126 [==============================] - 83s 656ms/step - loss: 0.0143 - acc: 0.9954 - val_loss: 0.1779 - val_acc: 0.9500\n",
      "Epoch 49/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.0209 - acc: 0.9927 - val_loss: 0.1565 - val_acc: 0.9587\n",
      "Epoch 50/500\n",
      "126/126 [==============================] - 83s 660ms/step - loss: 0.0185 - acc: 0.9957 - val_loss: 0.1605 - val_acc: 0.9580\n",
      "Epoch 51/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.0270 - acc: 0.9911 - val_loss: 0.1611 - val_acc: 0.9550\n",
      "Epoch 52/500\n",
      "126/126 [==============================] - 87s 690ms/step - loss: 0.0127 - acc: 0.9964 - val_loss: 0.1867 - val_acc: 0.9467\n",
      "Epoch 53/500\n",
      "126/126 [==============================] - 84s 663ms/step - loss: 0.0233 - acc: 0.9934 - val_loss: 0.1618 - val_acc: 0.9567\n",
      "Epoch 54/500\n",
      "126/126 [==============================] - 86s 684ms/step - loss: 0.0160 - acc: 0.9947 - val_loss: 0.1722 - val_acc: 0.9590\n",
      "Epoch 55/500\n",
      "126/126 [==============================] - 84s 665ms/step - loss: 0.0148 - acc: 0.9957 - val_loss: 0.1771 - val_acc: 0.9553\n",
      "Epoch 56/500\n",
      "126/126 [==============================] - 84s 670ms/step - loss: 0.0170 - acc: 0.9947 - val_loss: 0.1764 - val_acc: 0.9553\n",
      "Epoch 57/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.0181 - acc: 0.9931 - val_loss: 0.1717 - val_acc: 0.9570\n",
      "Epoch 58/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0168 - acc: 0.9931 - val_loss: 0.1597 - val_acc: 0.9553\n",
      "Epoch 59/500\n",
      "126/126 [==============================] - 86s 686ms/step - loss: 0.0197 - acc: 0.9934 - val_loss: 0.1673 - val_acc: 0.9583\n",
      "Epoch 60/500\n",
      "126/126 [==============================] - 84s 667ms/step - loss: 0.0059 - acc: 0.9987 - val_loss: 0.1559 - val_acc: 0.9610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/500\n",
      "126/126 [==============================] - 83s 660ms/step - loss: 0.0142 - acc: 0.9957 - val_loss: 0.1663 - val_acc: 0.9580\n",
      "Epoch 62/500\n",
      "126/126 [==============================] - 83s 660ms/step - loss: 0.0129 - acc: 0.9974 - val_loss: 0.1932 - val_acc: 0.9487\n",
      "Epoch 63/500\n",
      "126/126 [==============================] - 84s 664ms/step - loss: 0.0150 - acc: 0.9950 - val_loss: 0.1789 - val_acc: 0.9550\n",
      "Epoch 64/500\n",
      "126/126 [==============================] - 82s 653ms/step - loss: 0.0250 - acc: 0.9934 - val_loss: 0.1646 - val_acc: 0.9597\n",
      "Epoch 65/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0400 - acc: 0.9924 - val_loss: 0.1679 - val_acc: 0.9533\n",
      "Epoch 66/500\n",
      "126/126 [==============================] - 83s 656ms/step - loss: 0.0155 - acc: 0.9940 - val_loss: 0.1962 - val_acc: 0.9507\n",
      "Epoch 67/500\n",
      "126/126 [==============================] - 83s 660ms/step - loss: 0.0370 - acc: 0.9944 - val_loss: 0.1685 - val_acc: 0.9557\n",
      "Epoch 68/500\n",
      "126/126 [==============================] - 83s 661ms/step - loss: 0.0115 - acc: 0.9960 - val_loss: 0.1712 - val_acc: 0.9543\n",
      "Epoch 69/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0134 - acc: 0.9964 - val_loss: 0.1602 - val_acc: 0.9597\n",
      "Epoch 70/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.0134 - acc: 0.9950 - val_loss: 0.1469 - val_acc: 0.9640\n",
      "Epoch 71/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0085 - acc: 0.9987 - val_loss: 0.1555 - val_acc: 0.9640\n",
      "Epoch 72/500\n",
      "126/126 [==============================] - 83s 661ms/step - loss: 0.0100 - acc: 0.9970 - val_loss: 0.1860 - val_acc: 0.9507\n",
      "Epoch 73/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0079 - acc: 0.9977 - val_loss: 0.1527 - val_acc: 0.9603\n",
      "Epoch 74/500\n",
      "126/126 [==============================] - 85s 673ms/step - loss: 0.0101 - acc: 0.9967 - val_loss: 0.1605 - val_acc: 0.9623\n",
      "Epoch 75/500\n",
      "126/126 [==============================] - 85s 675ms/step - loss: 0.0103 - acc: 0.9964 - val_loss: 0.1642 - val_acc: 0.9613\n",
      "Epoch 76/500\n",
      "126/126 [==============================] - 85s 675ms/step - loss: 0.0109 - acc: 0.9974 - val_loss: 0.1653 - val_acc: 0.9613\n",
      "Epoch 77/500\n",
      "126/126 [==============================] - 85s 678ms/step - loss: 0.0113 - acc: 0.9974 - val_loss: 0.1550 - val_acc: 0.9637\n",
      "Epoch 78/500\n",
      "126/126 [==============================] - 85s 677ms/step - loss: 0.0122 - acc: 0.9977 - val_loss: 0.1542 - val_acc: 0.9637\n",
      "Epoch 79/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.0145 - acc: 0.9950 - val_loss: 0.1639 - val_acc: 0.9580\n",
      "Epoch 80/500\n",
      "126/126 [==============================] - 83s 661ms/step - loss: 0.0101 - acc: 0.9970 - val_loss: 0.1618 - val_acc: 0.9607\n",
      "Epoch 81/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0171 - acc: 0.9937 - val_loss: 0.1657 - val_acc: 0.9573\n",
      "Epoch 82/500\n",
      "126/126 [==============================] - 83s 656ms/step - loss: 0.0096 - acc: 0.9977 - val_loss: 0.1947 - val_acc: 0.9523\n",
      "Epoch 83/500\n",
      "126/126 [==============================] - 82s 653ms/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.1858 - val_acc: 0.9543\n",
      "Epoch 84/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.0169 - acc: 0.9944 - val_loss: 0.1875 - val_acc: 0.9507\n",
      "Epoch 85/500\n",
      "126/126 [==============================] - 82s 654ms/step - loss: 0.0129 - acc: 0.9957 - val_loss: 0.1593 - val_acc: 0.9593\n",
      "Epoch 86/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.1378 - val_acc: 0.9670\n",
      "Epoch 87/500\n",
      "126/126 [==============================] - 83s 660ms/step - loss: 0.0097 - acc: 0.9980 - val_loss: 0.1306 - val_acc: 0.9703\n",
      "Epoch 88/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0189 - acc: 0.9941 - val_loss: 0.1988 - val_acc: 0.9510\n",
      "Epoch 89/500\n",
      "126/126 [==============================] - 83s 657ms/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.1669 - val_acc: 0.9613\n",
      "Epoch 90/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.0135 - acc: 0.9921 - val_loss: 0.1543 - val_acc: 0.9667\n",
      "Epoch 91/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0111 - acc: 0.9977 - val_loss: 0.1427 - val_acc: 0.9660\n",
      "Epoch 92/500\n",
      "126/126 [==============================] - 83s 657ms/step - loss: 0.0068 - acc: 0.9987 - val_loss: 0.1633 - val_acc: 0.9633\n",
      "Epoch 93/500\n",
      "126/126 [==============================] - 84s 663ms/step - loss: 0.0064 - acc: 0.9983 - val_loss: 0.1613 - val_acc: 0.9663\n",
      "Epoch 94/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0187 - acc: 0.9960 - val_loss: 0.1479 - val_acc: 0.9650\n",
      "Epoch 95/500\n",
      "126/126 [==============================] - 84s 663ms/step - loss: 0.0101 - acc: 0.9974 - val_loss: 0.1559 - val_acc: 0.9637\n",
      "Epoch 96/500\n",
      "126/126 [==============================] - 85s 676ms/step - loss: 0.0071 - acc: 0.9977 - val_loss: 0.1430 - val_acc: 0.9667\n",
      "Epoch 97/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0272 - acc: 0.9957 - val_loss: 0.1494 - val_acc: 0.9620\n",
      "Epoch 98/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.0171 - acc: 0.9957 - val_loss: 0.1281 - val_acc: 0.9670\n",
      "Epoch 99/500\n",
      "126/126 [==============================] - 84s 666ms/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.1372 - val_acc: 0.9647\n",
      "Epoch 100/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.0076 - acc: 0.9974 - val_loss: 0.1425 - val_acc: 0.9660\n",
      "Epoch 101/500\n",
      "126/126 [==============================] - 83s 656ms/step - loss: 0.0061 - acc: 0.9980 - val_loss: 0.1476 - val_acc: 0.9643\n",
      "Epoch 102/500\n",
      "126/126 [==============================] - 84s 663ms/step - loss: 0.0066 - acc: 0.9990 - val_loss: 0.1398 - val_acc: 0.9653\n",
      "Epoch 103/500\n",
      "126/126 [==============================] - 83s 661ms/step - loss: 0.0155 - acc: 0.9941 - val_loss: 0.1444 - val_acc: 0.9647\n",
      "Epoch 104/500\n",
      "126/126 [==============================] - 83s 657ms/step - loss: 0.0053 - acc: 0.9980 - val_loss: 0.1380 - val_acc: 0.9680\n",
      "Epoch 105/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.0072 - acc: 0.9983 - val_loss: 0.1486 - val_acc: 0.9630\n",
      "Epoch 106/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.0059 - acc: 0.9990 - val_loss: 0.1324 - val_acc: 0.9663\n",
      "Epoch 107/500\n",
      "126/126 [==============================] - 83s 656ms/step - loss: 0.0041 - acc: 0.9987 - val_loss: 0.1267 - val_acc: 0.9687\n",
      "Epoch 108/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.0076 - acc: 0.9970 - val_loss: 0.1422 - val_acc: 0.9667\n",
      "Epoch 109/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0240 - acc: 0.9914 - val_loss: 0.1216 - val_acc: 0.9703\n",
      "Epoch 110/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.0062 - acc: 0.9987 - val_loss: 0.1281 - val_acc: 0.9657\n",
      "Epoch 111/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0082 - acc: 0.9967 - val_loss: 0.1361 - val_acc: 0.9647\n",
      "Epoch 112/500\n",
      "126/126 [==============================] - 82s 654ms/step - loss: 0.0226 - acc: 0.9941 - val_loss: 0.1331 - val_acc: 0.9650\n",
      "Epoch 113/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0040 - acc: 0.9987 - val_loss: 0.1505 - val_acc: 0.9620\n",
      "Epoch 114/500\n",
      "126/126 [==============================] - 85s 672ms/step - loss: 0.0055 - acc: 0.9987 - val_loss: 0.1657 - val_acc: 0.9603\n",
      "Epoch 115/500\n",
      "126/126 [==============================] - 85s 675ms/step - loss: 0.0076 - acc: 0.9970 - val_loss: 0.1490 - val_acc: 0.9660\n",
      "Epoch 116/500\n",
      "126/126 [==============================] - 84s 666ms/step - loss: 0.0048 - acc: 0.9993 - val_loss: 0.1307 - val_acc: 0.9663\n",
      "Epoch 117/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0254 - acc: 0.9924 - val_loss: 0.1442 - val_acc: 0.9633\n",
      "Epoch 118/500\n",
      "126/126 [==============================] - 84s 663ms/step - loss: 0.0062 - acc: 0.9983 - val_loss: 0.1296 - val_acc: 0.9687\n",
      "Epoch 119/500\n",
      "126/126 [==============================] - 84s 668ms/step - loss: 0.0056 - acc: 0.9990 - val_loss: 0.1394 - val_acc: 0.9670\n",
      "Epoch 120/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 82s 651ms/step - loss: 0.0129 - acc: 0.9941 - val_loss: 0.1781 - val_acc: 0.9550\n",
      "Epoch 121/500\n",
      "126/126 [==============================] - 86s 685ms/step - loss: 0.0062 - acc: 0.9990 - val_loss: 0.1394 - val_acc: 0.9667\n",
      "Epoch 122/500\n",
      "126/126 [==============================] - 86s 682ms/step - loss: 0.0060 - acc: 0.9980 - val_loss: 0.1406 - val_acc: 0.9673\n",
      "Epoch 123/500\n",
      "126/126 [==============================] - 82s 650ms/step - loss: 0.0049 - acc: 0.9983 - val_loss: 0.1378 - val_acc: 0.9680\n",
      "Epoch 124/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0106 - acc: 0.9974 - val_loss: 0.1302 - val_acc: 0.9693\n",
      "Epoch 125/500\n",
      "126/126 [==============================] - 84s 664ms/step - loss: 0.0133 - acc: 0.9921 - val_loss: 0.1610 - val_acc: 0.9597\n",
      "Epoch 126/500\n",
      "126/126 [==============================] - 84s 663ms/step - loss: 0.0171 - acc: 0.9954 - val_loss: 0.1366 - val_acc: 0.9687\n",
      "Epoch 127/500\n",
      "126/126 [==============================] - 83s 660ms/step - loss: 0.0043 - acc: 0.9993 - val_loss: 0.1170 - val_acc: 0.9707\n",
      "Epoch 128/500\n",
      "126/126 [==============================] - 83s 661ms/step - loss: 0.0253 - acc: 0.9957 - val_loss: 0.1109 - val_acc: 0.9727\n",
      "Epoch 129/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.0072 - acc: 0.9990 - val_loss: 0.1339 - val_acc: 0.9653\n",
      "Epoch 130/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0047 - acc: 0.9990 - val_loss: 0.1262 - val_acc: 0.9720\n",
      "Epoch 131/500\n",
      "126/126 [==============================] - 82s 652ms/step - loss: 0.0049 - acc: 0.9990 - val_loss: 0.1166 - val_acc: 0.9730\n",
      "Epoch 132/500\n",
      "126/126 [==============================] - 83s 658ms/step - loss: 0.0056 - acc: 0.9983 - val_loss: 0.1286 - val_acc: 0.9683\n",
      "Epoch 133/500\n",
      "126/126 [==============================] - 86s 686ms/step - loss: 0.0060 - acc: 0.9987 - val_loss: 0.1129 - val_acc: 0.9710\n",
      "Epoch 134/500\n",
      "126/126 [==============================] - 86s 679ms/step - loss: 0.0359 - acc: 0.9934 - val_loss: 0.1492 - val_acc: 0.9617\n",
      "Epoch 135/500\n",
      "126/126 [==============================] - 87s 687ms/step - loss: 0.0044 - acc: 0.9987 - val_loss: 0.1243 - val_acc: 0.9700\n",
      "Epoch 136/500\n",
      "126/126 [==============================] - 86s 682ms/step - loss: 0.0071 - acc: 0.9980 - val_loss: 0.1233 - val_acc: 0.9687\n",
      "Epoch 137/500\n",
      "126/126 [==============================] - 87s 687ms/step - loss: 0.0094 - acc: 0.9974 - val_loss: 0.1201 - val_acc: 0.9707\n",
      "Epoch 138/500\n",
      "126/126 [==============================] - 86s 682ms/step - loss: 0.0053 - acc: 0.9983 - val_loss: 0.1186 - val_acc: 0.9690\n",
      "Epoch 139/500\n",
      "126/126 [==============================] - 86s 684ms/step - loss: 0.0240 - acc: 0.9898 - val_loss: 0.1239 - val_acc: 0.9667\n",
      "Epoch 140/500\n",
      "126/126 [==============================] - 87s 690ms/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.1116 - val_acc: 0.9727\n",
      "Epoch 141/500\n",
      "126/126 [==============================] - 86s 681ms/step - loss: 0.0096 - acc: 0.9974 - val_loss: 0.1124 - val_acc: 0.9710\n",
      "Epoch 142/500\n",
      "126/126 [==============================] - 87s 688ms/step - loss: 0.0232 - acc: 0.9928 - val_loss: 0.1206 - val_acc: 0.9743\n",
      "Epoch 143/500\n",
      "126/126 [==============================] - 86s 684ms/step - loss: 0.0077 - acc: 0.9974 - val_loss: 0.1137 - val_acc: 0.9723\n",
      "Epoch 144/500\n",
      "126/126 [==============================] - 83s 662ms/step - loss: 0.0206 - acc: 0.9964 - val_loss: 0.1190 - val_acc: 0.9713\n",
      "Epoch 145/500\n",
      "126/126 [==============================] - 82s 653ms/step - loss: 0.0059 - acc: 0.9980 - val_loss: 0.1004 - val_acc: 0.9737\n",
      "Epoch 146/500\n",
      "126/126 [==============================] - 82s 651ms/step - loss: 0.0053 - acc: 0.9987 - val_loss: 0.1122 - val_acc: 0.9730\n",
      "Epoch 147/500\n",
      "126/126 [==============================] - 82s 651ms/step - loss: 0.0150 - acc: 0.9960 - val_loss: 0.0999 - val_acc: 0.9747\n",
      "Epoch 148/500\n",
      "126/126 [==============================] - 83s 660ms/step - loss: 0.0069 - acc: 0.9960 - val_loss: 0.1033 - val_acc: 0.9757\n",
      "Epoch 149/500\n",
      "126/126 [==============================] - 83s 657ms/step - loss: 0.0045 - acc: 0.9987 - val_loss: 0.1069 - val_acc: 0.9743\n",
      "Epoch 150/500\n",
      "126/126 [==============================] - 82s 655ms/step - loss: 0.0158 - acc: 0.9967 - val_loss: 0.1034 - val_acc: 0.9737\n",
      "Epoch 151/500\n",
      "126/126 [==============================] - 83s 659ms/step - loss: 0.0037 - acc: 0.9993 - val_loss: 0.1067 - val_acc: 0.9760\n",
      "Epoch 152/500\n",
      "126/126 [==============================] - 84s 670ms/step - loss: 0.0067 - acc: 0.9983 - val_loss: 0.1219 - val_acc: 0.9727\n",
      "Epoch 153/500\n",
      " 23/126 [====>.........................] - ETA: 40s - loss: 0.0037 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-940a4f20f027>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                         callbacks=[cp_callback])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2175\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2177\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   2178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         outs = model.train_on_batch(\n\u001b[0;32m--> 176\u001b[0;31m             x, y, sample_weight=sample_weight, class_weight=class_weight)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ecomed/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train model\n",
    "with tf.device('/gpu:3'):\n",
    "    model.fit_generator(generator=train_generator,\n",
    "                        validation_data=validation_generator, \n",
    "                        epochs=500,\n",
    "                        callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: ['train']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: ['eval']\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/bowen/Documents/Ecomed/models/temp-b'1551769465'/saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'/home/bowen/Documents/Ecomed/models/1551769465'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.contrib.saved_model.save_keras_model(model, home_path + 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CNN_Model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
